{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a16191b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Homework 8: (a) Posterior Predictive Distributions<br> and (b) Missing Data Imputation\n",
    "\n",
    "### 1. Describe how the posterior predictive distribution is created for mixture models \n",
    "\n",
    "### 2. Describe how the posterior predictive distribution is created in general\n",
    "\n",
    "### 3. Have glance through [this](https://www.pymc.io/projects/examples/en/latest/case_studies/Missing_Data_Imputation.html) and then describe how, if you were doing a regression of $y$ on $X$ but $X$ had some missing values, you could perform a Bayesian analysis without throwing away the rows with missing values in $X$\n",
    "\n",
    "- **Hint: latent variables $v$ indicating the subpopulation are competely missing values that we simply treat as paramters to be inferred though posterior analysis... the same sort of thing can be done with missing values in data that need to be imputed... we should just be careful about the MCAR assumption...**\n",
    "\n",
    "### 4. Work on your course project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f40cb",
   "metadata": {},
   "source": [
    "1. As written in the code above, we calculate the average posterior predictive distribution by repeatedly drawing from the chains. The draw defines a normal distribution (or distribution used in the mixture model), which is weighted by the likelihood of chosing this distribution in the mixture model. The average of these draws is the posterior predictive distribution.\n",
    "\n",
    "2. The posterior predictive distribution is created by drawing from the posterior distribution of the parameters and the likelihood of the data given the parameters. The average of these draws is the posterior predictive distribution. This is equivalent to the Markov estimate of the integral, marginalizing out the parameters to get the posterior distribution.\n",
    "\n",
    "3. We introduce a new Bernouille random variable $v_i$ for each observation $i$ if the observation is missing or not. We then create a model for the missing data $X$ that is conditioned on the observed data. The model then computes the potential of the variable. The model is then samples using MCMC and we can then sample from the posterior distribution of the missing data given the observed data and the missing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a80b38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
